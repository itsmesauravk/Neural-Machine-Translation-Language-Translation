{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec727b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sentencepiece as spm\n",
    "from encoder_rnn import Encoder\n",
    "from decoder_rnn import Decoder\n",
    "from seq2seq_rnn import Seq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d03d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_en = spm.SentencePieceProcessor()\n",
    "sp_en.load(\"../data/processed/spm_en.model\") \n",
    "\n",
    "sp_de = spm.SentencePieceProcessor()\n",
    "sp_de.load(\"../data/processed/spm_de.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209eb5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd6a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 16000\n",
    "OUTPUT_DIM = 16000\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf9b138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(16000, 256)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (rnn): GRU(256, 512, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(16000, 256)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (rnn): GRU(256, 512, batch_first=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=16000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"seq2seq_gru_model.pt\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0773a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, sp_en, sp_de, device, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = sp_en.encode(sentence, out_type=int)\n",
    "    tokens = [2] + tokens + [3]  # BOS and EOS tokens\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)  # batch_size=1, seq_len=...\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(src_tensor)\n",
    "\n",
    "    trg_indexes = [2]  # BOS token\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden)\n",
    "            pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == 3:  # EOS token\n",
    "            break\n",
    "\n",
    "    translated_text = sp_de.decode(trg_indexes[1:-1])  # remove BOS and EOS\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b509001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: I love you more\n",
      "German: normalerweise lieben dich mehr.\n"
     ]
    }
   ],
   "source": [
    "english_sentence = \"I love you more\"\n",
    "translation = translate_sentence(english_sentence, model, sp_en, sp_de, device)\n",
    "print(\"English:\", english_sentence)\n",
    "print(\"German:\", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "761d2695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: I love you more\n",
      "German: mehr lieben sie lieben lieben!\n"
     ]
    }
   ],
   "source": [
    "english_sentence = \"I love you more\"\n",
    "translation = translate_sentence(english_sentence, model, sp_en, sp_de, device)\n",
    "print(\"English:\", english_sentence)\n",
    "print(\"German:\", translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ac0ba",
   "metadata": {},
   "source": [
    "BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba288c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'D:\\AI Projects\\LangConv\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Requirement already satisfied: joblib in d:\\ai projects\\langconv\\venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: colorama in d:\\ai projects\\langconv\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.2.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecff120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dae4270c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>German</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English      German\n",
       "0     Go.        Geh.\n",
       "1     Hi.      Hallo!\n",
       "2     Hi.  Grüß Gott!\n",
       "3    Run!       Lauf!\n",
       "4    Run.       Lauf!"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your file\n",
    "file_path = \"../data/raw/deu-eng/deu.txt\"\n",
    "\n",
    "# Read the file, split by tabs, and keep only the first two columns (English and German)\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", header=None, usecols=[0,1], names=[\"English\", \"German\"])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21745d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>German</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She made her appearance around noon.</td>\n",
       "      <td>Sie hat sich gegen Mittag blicken lassen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was about to call you.</td>\n",
       "      <td>Ich wollte Sie gerade anrufen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tom pretended that he was listening.</td>\n",
       "      <td>Tom tat so, als hörte er zu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let's cross the street.</td>\n",
       "      <td>Gehen wir über die Straße!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's said that he knows the secret.</td>\n",
       "      <td>Man sagt, er kenne das Geheimnis.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                English  \\\n",
       "0  She made her appearance around noon.   \n",
       "1              I was about to call you.   \n",
       "2  Tom pretended that he was listening.   \n",
       "3               Let's cross the street.   \n",
       "4   It's said that he knows the secret.   \n",
       "\n",
       "                                      German  \n",
       "0  Sie hat sich gegen Mittag blicken lassen.  \n",
       "1             Ich wollte Sie gerade anrufen.  \n",
       "2               Tom tat so, als hörte er zu.  \n",
       "3                 Gehen wir über die Straße!  \n",
       "4          Man sagt, er kenne das Geheimnis.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df.sample(2500, random_state=42)  # Randomly sample 2000 rows\n",
    "new_df.reset_index(drop=True, inplace=True)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e99331db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She made her appearance around noon.', 'I was about to call you.', 'Tom pretended that he was listening.']\n",
      "['Sie hat sich gegen Mittag blicken lassen.', 'Ich wollte Sie gerade anrufen.', 'Tom tat so, als hörte er zu.']\n"
     ]
    }
   ],
   "source": [
    "test_sentences = new_df['English'].tolist()  # list of English sentences (source)\n",
    "reference_sentences = new_df['German'].tolist()  # list of German sentences (reference)\n",
    "\n",
    "print(test_sentences[:3])\n",
    "\n",
    "print(reference_sentences[:3])\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6235a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentences(sentences, model, sp_en, sp_de, device):\n",
    "    translations = []\n",
    "    for sent in sentences:\n",
    "        # print(f\"Translating: {sent}\")\n",
    "        translation = translate_sentence(sent, model, sp_en, sp_de, device)\n",
    "        # print(f\"Translation: {translation}\")\n",
    "        translations.append(translation)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1bbee3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She made her appearance around noon.',\n",
       " 'I was about to call you.',\n",
       " 'Tom pretended that he was listening.']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = new_df['English'].tolist()  # list of English sentences (source)\n",
    "input_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "248f788e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sie hat sich gegen Mittag blicken lassen.'], ['Ich wollte Sie gerade anrufen.'], ['Tom tat so, als hörte er zu.']]\n"
     ]
    }
   ],
   "source": [
    "#making References\n",
    "# Original list\n",
    "refs = new_df['German'].tolist()\n",
    "\n",
    "# Convert to list of lists (each inner list with one reference string)\n",
    "references = [[ref] for ref in refs]\n",
    "\n",
    "print(references[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c60491d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates: ['um 1 ⁇  hat man sich um 1 ⁇ .', 'um es war ⁇  uhr.', 'als ha ha ha ha haes haielt als er sich das rennen würden.']\n"
     ]
    }
   ],
   "source": [
    "# candidates are model translations\n",
    "\n",
    "candidates = translate_sentences(input_data, model, sp_en, sp_de, device)\n",
    "print(\"Candidates:\", candidates[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcfcf4",
   "metadata": {},
   "source": [
    "### **BLEU, METEOR, and chrF (with NLTK and SacreBLEU)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9951db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad518858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sacrebleu.metrics import CHRF\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize (basic whitespace tokenization)\n",
    "tokenized_refs = [[ref.split() for ref in refs] for refs in references]\n",
    "tokenized_cands = [cand.split() for cand in candidates]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "66cc665f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram BLEU score: 0.5732\n",
      "2-gram BLEU score: 0.3976\n",
      "3-gram BLEU score: 0.2997\n",
      "4-gram BLEU score: 0.2322\n"
     ]
    }
   ],
   "source": [
    "# 1. BLEU Score\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "# Calculate BLEU with weights to only consider 1-gram\n",
    "bleu_1gram = corpus_bleu(references, candidates, weights=(1, 0, 0, 0) , smoothing_function=smooth)\n",
    "bleu_2gram = corpus_bleu(references, candidates, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
    "bleu_3gram = corpus_bleu(references, candidates, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth)\n",
    "bleu_4gram = corpus_bleu(references, candidates, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "\n",
    "print(f\"1-gram BLEU score: {bleu_1gram:.4f}\")\n",
    "print(f\"2-gram BLEU score: {bleu_2gram:.4f}\")\n",
    "print(f\"3-gram BLEU score: {bleu_3gram:.4f}\")\n",
    "print(f\"4-gram BLEU score: {bleu_4gram:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "184b31f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHRF SCORE: 32.1375\n"
     ]
    }
   ],
   "source": [
    "# 2. CHRF Score\n",
    "\n",
    "from sacrebleu.metrics import CHRF\n",
    "\n",
    "\n",
    "chrf_metric = CHRF()\n",
    "chrf_score = chrf_metric.corpus_score(candidates, references)\n",
    "print(f\"CHRF SCORE: {chrf_score.score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61788e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
